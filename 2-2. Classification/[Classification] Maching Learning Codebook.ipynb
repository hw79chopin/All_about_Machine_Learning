{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Classification] 혀누를 위한 Maching Learning",
      "provenance": [],
      "collapsed_sections": [
        "whltSFA-WtX1",
        "uuDiyGB6XZQm"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFuejL5sXMt"
      },
      "source": [
        "---\n",
        "# 📁 Hyun's Code collection (Classification) \n",
        "---\n",
        "\n",
        "\n",
        "### <h3 align=\"right\">🥇 Authored by <strong>Hyun</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whltSFA-WtX1"
      },
      "source": [
        "# ✏️ What is **Classification**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziKN21X-nRmi"
      },
      "source": [
        "- Supervised Learning의 한 종류\n",
        "- 기존 데이터에 대한 특징을 학습한 뒤, 새로운 데이터가 들어왔을 때 그 데이터의 적합한 집단에 따라 분류해주는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuDiyGB6XZQm"
      },
      "source": [
        "# ✏️ Classification Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvXHGMn6UOXk"
      },
      "source": [
        "## 🔎 Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyosm9OZw9qZ"
      },
      "source": [
        "- Sample이 특정 class에 속할 확률을 추정하는 데 사용하는 Regression\n",
        "- 선형 회귀와 같이 input에 대한 parameter를 계산한 뒤, 결괏값의 Logistic을 출력\n",
        "- Logistic Regression의 확률 추정 <br>\n",
        "<br> $\\hat{p} = h_\\theta(x) = \\sigma(\\theta^T*x)$ <br>\n",
        "- [Logistic Regression numpy 구현](https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece)\n",
        "\n",
        "- Logistic function은 $\\sigma(t) = \\frac{1}{1+exp(-t)}$\n",
        "<br>\n",
        "\n",
        "<h3> Logistic function의 cost function (log loss)  </h3>\n",
        "\n",
        "- $J(\\theta) = -\\frac{1}{m}\\sum_{(i=1)}^m[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$ <br>\n",
        "  <br>이 비용 함수의 최솟값을 계산하는 해는 없음.  \n",
        "  GD를 사용해서 전역 최솟값을 찾아야 함<br>\n",
        "<br>\n",
        "\n",
        "<h3> Logistic function cost function의 편도함수 </h3>\n",
        "\n",
        "- $\\frac{\\sigma}{\\sigma\\theta_j}J(\\theta) = \\frac{1}{m}\\sum_{(i=1)}^m(\\sigma(\\theta^T*x^{(i)})-y^{(i)})x_j^{(i)}$\n",
        "<br>\n",
        "- Logistic Regression의 cost function은 볼록함수여서 GD를 써도 지역 최솟값에 갇히지 않는다.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PToCO4fWVl18"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlc5Mxl1mgmq"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "model = sm.Logit(y_train, X_train)\n",
        "results = model.fit()\n",
        "results.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWb1GnWYmgPP"
      },
      "source": [
        "  # Logistic regression 에시\n",
        "# 먼저 데이터를 반응변인과 예측변인으로 분리하기\n",
        "Y = (df['Status'] == 'Developed').astype(np.int) #'Developed'면 1, 아니면 0으로\n",
        "X = df.drop('Status', axis=1)\n",
        "\n",
        "# 예측변인과 반응변인을 train set과 test set으로 분리해 줍니다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. 먼저 모델을 만들어 줍니다.\n",
        "logReg = LogisticRegression()\n",
        "\n",
        "# 2. 모델 적합시킵니다.\n",
        "logReg.fit(X_train, y_train)\n",
        "\n",
        "# 3. 트레이닝셋과 테스트셋에서의 성능을 확인합니다. (이 경우 결정계수 R^2)\n",
        "print('Training R^2:', logReg.score(X_train, y_train))\n",
        "print('Test R^2: ', logReg.score(X_test, y_test))\n",
        "\n",
        "# 모델로부터 반응을 예측하고 accuracy, f1-score, confusion matrix를 구해봅시다.\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "y_pred = logReg.predict(X_test)\n",
        "print(\"Accuracy: %.2f\" %accuracy_score(y_test, y_pred))\n",
        "print(\"F1 score: %.2f\" %f1_score(y_test, y_pred))\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKTkFrM4Xfks"
      },
      "source": [
        "## 🔎 Linear Support Vector Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0uA9qcpf_pY"
      },
      "source": [
        "\n",
        "- SVM는 클래스 사이에 폭이 가장 넓은 도로를 찾는 것이다! 그래야 새로운 샘플에 대해서도 잘 작동!\n",
        "- Large Margin Classification이라고도 부른다.\n",
        "\n",
        "- SVC는 기본값에서 클래스 확률을 제공하지 않음. 그렇기에 probability 매개변수를 True로 지정해줘야 함!\n",
        "- 그러면 cross-validation을 쓰기 때문에 느려지지만 predict_proba() 메소드 사용 가능  <br>\n",
        " \n",
        "- SVM은 특성의 scale에 민감\n",
        "\n",
        "> Hard margin Classification\n",
        "- 문제\n",
        "  1. 데이터가 선형적으로 구분되어야 잘 작동\n",
        "  2. 이상치에 민감\n",
        "\n",
        "> Soft Margin Classification\n",
        "- 도로의 폭은 가능한 넓게 하는 것 & 마진 오류 사이에 균형을 잡은 모델\n",
        "- SVM 모델의 C hyperparameter로 균형을 조절 가능하다!  \n",
        "  C가 낮을 수록 더 일반화된 모델을 만들 수 있다. (p.203)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HQFxPX7XtF2"
      },
      "source": [
        "### 📔 Basic Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H3daCvIXvWE"
      },
      "source": [
        "- **LinearSVC( C=1, loss='hinge')**: 기본 learner SVC 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM75jOOwXxgX"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfLegX505h-4"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = LinearSVC(C=1, loss='hinge')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1hWbzTXhoH"
      },
      "source": [
        "## 🔎 Non-linear Support Vector Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN7vsBr66CMs"
      },
      "source": [
        "- 비선형 데이터셋을 다루는 한 가지 방법은 다항 특성 등을 추가하는 것\n",
        "- 아니면 커널 트릭으로 훈련가능. 커널 트릭은 실제로는 특성을 추가하지 않으면서 다항식 특성을 많이 추가한 것과 같은 결과를 얻을 수 있음\n",
        "- 혹은 유사도 함수로 계산한 특성을 추가할 수도 있다. 훈련 세트가 너무 크지 않다면 가우시안 RBF 커널을 시도해도 좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2tt2wx2YY6s"
      },
      "source": [
        "### 📔 Basic Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXHLsnqJYd23"
      },
      "source": [
        "- **LinearSVC( C = 1, kernel = \"linear\", loss = 'hinge' )**: 기본 learner SVC 모델\n",
        "- **LinearSVC( C = 5, kernel = \"poly\", degree = 3, coef0 = 1 )**: 다항 kernel이 추가된 SVM 모델\n",
        "  - kernel: poly로 설정하면 다항식 커널을 사용해 SVM 분류기를 훈련시킴\n",
        "  - coef0: 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지 조절함\n",
        "- **LinearSVC( C = 0.001, kernel = \"rbf\", gamma = 5 )**: 기본 learner SVC 모델\n",
        "  - gamma: gamma를 증가시키면 종 모양 그래프가 커져 각 샘플의 영향 범위가 작아짐. 규제 역할을 함. Overfitting이면 감소시키고 Underfitting이면 증가시켜야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bJS_3BEYY2Z"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btAzJ_Ja7gaS"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "polynomial_svm_clf = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", max_iter=2000, random_state=42))\n",
        "    ])\n",
        "\n",
        "polynomial_svm_clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv8Lzj608B-g"
      },
      "source": [
        "# 그림으로 확인하기\n",
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
        "\n",
        "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
        "\n",
        "save_fig(\"moons_polynomial_svc_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLxLjwhZXJSB"
      },
      "source": [
        "## 🔎 SGD Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gRZo1oYipG"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUZD9-BvnO3E"
      },
      "source": [
        "# SGD Classifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf=SGDClassifier(random_state=42) \n",
        "'''\n",
        "GDClassifier는 randomness에 기반하기 때문에, \n",
        "고정 값을 정하고 싶다면, random_state parameter를 쓰자.\n",
        "'''\n",
        "sgd_clf.fit(X_train,y_train)\n",
        "\n",
        "sgd_clf.predict(X_test)\n",
        "\n",
        "y_pred = sgd_clf.predict(X_test)\n",
        "\n",
        "f1_score(y_test,y_pred, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sisl4ELKXlvI"
      },
      "source": [
        "## 🔎 XGboost Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdq9oYxmYjDI"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z84bRmfnQQG"
      },
      "source": [
        "# Xgboost Classifier\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "model.predict(X_test)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1_score(y_test,y_pred, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}