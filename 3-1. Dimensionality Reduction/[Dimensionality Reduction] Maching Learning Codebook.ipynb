{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Dimensionality Reduction] 혀누를 위한 Maching Learning",
      "provenance": [],
      "collapsed_sections": [
        "RYFuejL5sXMt",
        "QWwdYMefcUfO",
        "DfdUtwXGce4p",
        "Po4dl7gLcjWG",
        "9hyWVphmddNr",
        "5v3P7SJkdCGE",
        "MJIHkAqKdtEs",
        "v0DILaYQfYzy",
        "amHFx7ymf633"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFuejL5sXMt"
      },
      "source": [
        "---\n",
        "# 📁 Hyun's Code collection (Dimensionality Reduction) \n",
        "---\n",
        "\n",
        "\n",
        "### <h3 align=\"right\">🥇 Authored by <strong>Hyun</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwdYMefcUfO"
      },
      "source": [
        "# ✏️ What is **Dimensinality Reduction**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKJA1Xz6hsW1"
      },
      "source": [
        "- 차원의 저주를 해결하기 위해 feature를 줄여주는 과정.\n",
        "> 목적\n",
        " -  훈련 알고리즘 속도 향상\n",
        " - 데이터 시각화 및 insight 도출\n",
        " - 메모리 공간을 절약\n",
        "> 단점\n",
        " - 일부 정보를 손실해 성능 감소 우려\n",
        " - 계산 비용이 높다\n",
        " - 파이프라인의 복잡도 증가\n",
        " - 변환된 데이터를 해석하지 못함\n",
        "- 차원 축소는 지도 학습의 준비단계라고 할 수 있다. \n",
        "- Feature를 줄이면 속도는 빨라지지만, 정보가 줄어들기 때문에 약간의 성능 저하가 있을 수 있어서 차원 축소를 하기 전에 먼저 Original data를 training 시켜보는 것을 추천!\n",
        "- 모델을 학습시키기 전에 train data의 차원을 감소시키면 학습 속도는 빨라지지만 모델의 성능은 항상 더 낫거나 간단한 모델이 되는 것은 아니다. 이것은 데이터셋이 어떠한 모양을 하고 있느냐에 따라 달라진다.\n",
        "- 사람들은 2차원까지만 생각할 수 있기 때문에 데이터 시각화에도 유용하다.\n",
        "- 차원 축소는 대표적으로 ```PCA```와 ```LLE```가 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4bSWcyYcaGm"
      },
      "source": [
        "<h2> Curse of Dimensionality </h2>\n",
        "\n",
        "- 차원의 저주란 저차원 공간에는 없는 많은 문제가 고차원 공간에서 일어난다는 사실\n",
        "- 차원이 커지면 커질수록 데이터 간의 거리가 멀어진다. 그래서 그래프 상으로 보게 되면 엄청 떨어져 있는 모습을 볼 수 있다. \n",
        "- 이 상태로 모델을 돌리면 overfitting 될 가능성이 크다. \n",
        "- Overfitting을 해결하는 첫번째 방법은 feature만큼 row를 늘려줘서 density를 올려주는 방법이 있는데, 이는 row가 너무 많아질 가능성이 있기 때문에 좋은 방법은 아니다. 그래서 가장 많이 사용하는 방법은 Projection과 Manifold 방식이다. \n",
        "\n",
        "![123123](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile1.uf.tistory.com%2Fimage%2F99FF9F335B8A484A31820B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfdUtwXGce4p"
      },
      "source": [
        "# ✏️ Dimensinality Reduction Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po4dl7gLcjWG"
      },
      "source": [
        "## 🔎 PCA (Principal Component Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUv7EQZOhveI"
      },
      "source": [
        "- PCA(Principal Component Analysis)는 주성분 분석이라고도 하며 고차원 데이터 집합이 주어졌을 때 원래의 고차원 데이터와 가장 비슷하면서 더 낮은 차원 데이터를 찾아내는 방법\n",
        "- 차원축소 방법이다. 최적의 초평면을 찾고, 그 다음에 data를 그 초평면에 projection 시켜주는 방식이다. \n",
        "\n",
        "<h2> 분산보존 </h2>\n",
        "\n",
        "- 최적의 lower-dimension hyperplane을 찾는 기준 중 하나는 분산보존이다. \n",
        "- PCA는 데이터의 분산이 최대가 되는 축을 찾는다. \n",
        "- 즉, 원본 데이터셋과 투영된 데이터셋 간의 평균제곱거리를 최소화 하는 축을 찾는 것이다. \n",
        "- 밑에 그림을 보면 C1을 축으로 한 데이터의 분산이 가장 잘 보존되어 있는걸 볼 수 있다.\n",
        "![123123](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile28.uf.tistory.com%2Fimage%2F99AC093E5B8A4904213CC3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba6fILXc85_"
      },
      "source": [
        "<h3> 📔 How PCA works </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFQhIOSSc3X_"
      },
      "source": [
        "PCA는 다음과 같은 단계로 이루어진다. \n",
        "1. 학습 데이터셋에서 분산이 최대인 축(axis)을 찾는다. \n",
        "2. 이렇게 찾은 첫번째 축과 직교(orthogonal)하면서 분산이 최대인 두 번째 축을 찾는다.\n",
        "3. 첫 번째 축과 두 번째 축에 직교하고 분산을 최대한 보존하는 세 번째 축을 찾는다.\n",
        "4. 1~3과 같은 방법으로 데이터셋의 차원(특성 수)만큼의 축을 찾는다.\n",
        "PC의 방향은 항상 일정하지 않다. train set을 살짝 조정해서 PCA를 실행시키면, 새로운 PC가 원래의 PC랑 반대에 있을 가능성이 있다. 그러나 축이나 평면은 같다.\n",
        "![123123](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile6.uf.tistory.com%2Fimage%2F996F65335B8A493207D19B)\n",
        "- PC를 찾을 때 Singular Value Decomposition(SVD)를 사용해서 원래의 training set matrix를 3개의 matrices로 분해시켜준다.\n",
        "<br>\n",
        "- PCA는 dataset이 origin 주변으로 center 되어있다고 가정한다.\n",
        "- Scikit-learn에서는 이 부분을 자동으로 만들어주지만, 아닐 경우 이 부분에 대해 고려해주어야 한다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc58MrhPc3bs"
      },
      "source": [
        "<h3>  📔 PCA for Data Compression </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bodRScF7c3fA"
      },
      "source": [
        "- PCA를 통해 없어진 변수는 다시 살릴 수 있다. \n",
        "- 그러나 원래 데이터와 완전 똑같이 복원할 순 없다!ㅠ.ㅠ\n",
        "- Original과 Recover 된 데이터 사이의 mean square distance를 reconstruction error라고 부른다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVGDVmo2c3iR"
      },
      "source": [
        "<h3> 📔 Types of PCA </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPvnRwVUc3lN"
      },
      "source": [
        "<h4> 🎈 Randomized PCA </h4>\n",
        "\n",
        "- Stochastic algorithm을 사용하는 PCA 방법\n",
        "- scikit_learn에서 hyperparameter 중에서 svd_solver를 'randomzied'로 설정하면 된다.\n",
        "- Randomized PCA를 사용하면 처음 d개의 주성분을 대략적으로 찾을 수 있다. \n",
        "- d가 n개보다 훨씬 적게 되면 매우 빠른 성능을 볼 수 있다. 이 함수에 'auto'로 설정하면 n 값이 500개 이상이고 d 값이 n 값의 80%보다 적으면 자동으로 randomized로 진행되고, 그 외에는 full svd가 진행된다. \n",
        "\n",
        "<h4> 🎈 Incremental PCA (IPCA) </h4>\n",
        "\n",
        "- IPCA는 train data를 mini-batch로 나눈 뒤 IPCA 알고리즘에 하나의 mini-batch를 input으로 넣어준다.\n",
        "- 모든 train data를 memory에 fit 시켜줘야한다는 PCA의 문제를 해결해줌!\n",
        "- IPCA는 학습 데이터셋이 클때 유용하다.\n",
        "\n",
        "<h4> 🎈 Kernel PCA (KPCA) </h4>\n",
        "\n",
        "- non-linear 투영을 사용해서 차원을 축소하는 방법\n",
        "- SVM에서 데이터를 저차원에서 선형 고차원으로 mapping 시켜 비선형 데이터셋에 SVM을 적용시키는 방식이라 생각하면 됨!\n",
        "- 이 방식은 projection 후 데이터들의 cluster를 유지시키는데 좋고, unrolling dataset에도 사용 가능하다. (twisted manifold와 매우 비슷) \n",
        "- KPCA는 unsupervised learning이기 때문에 어떤 hyperparameter와 어떤 kernel이 best인지 명확하게 알 수 없지만 GridSearch를 이용할 수 있다!\n",
        "- KernelPCA에서도 inverse로 원래 데이터 형태로 돌릴 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hyWVphmddNr"
      },
      "source": [
        "<h3> 📔 Appropriate PCA methods </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY7zkaLZdfgZ"
      },
      "source": [
        "> 기본 PCA\n",
        " - 우선적으로 사용, 데이터셋 크기가 메모리에 맞을 때에 가능\n",
        "\n",
        "> Incremental PCA\n",
        " - 대용량 데이터셋에 적합\n",
        "\n",
        "> Randomized PCA\n",
        " - 데이터셋이 메모리 크기에 맞고 차원을 크게 축소시킬 때 사용\n",
        "\n",
        "> Kernel PCA\n",
        " - 비선형 데이터셋에 유용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v3P7SJkdCGE"
      },
      "source": [
        "<h3> 📔 Basic Methods </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaRTQrJ8c5Qq"
      },
      "source": [
        "- **PCA ( n_components = 정수 )** : 기본 사용방법\n",
        "- **.fit_transform()**: 특정행렬을 낮은 차원의 근사행렬로 변환\n",
        "- **.inverse_transform()**: 변환된 근사행렬을 원래의 차원으로 복귀. 역변환 수행해서 재구성 오차를 측정해서 알고리즘의 성능을 평가\n",
        "- **.mean_**: 데이터의 평균값을 반환\n",
        "- **.components_**: 단위기저벡터를 반환\n",
        "- **.explained_variance_ratio**: 각 주성분 축에 몇 %의 분산이 보존되어있는지 보여줌\n",
        "- **.inverse_transform()**: 압축했던 data를 다시 예전 차원으로 복원"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9JL9ioc7trO"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_3jYnJqkF08"
      },
      "source": [
        "## 분산으로도 적절한 차원의 수를 구할 수 있음\n",
        "from tensorflow.keras.datasets import mnist\n",
        "​\n",
        "# MNIST load\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "​\n",
        "# reshape\n",
        "train_x = train_x.reshape(-1, 28*28) \n",
        "​\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(train_x)  # PCA 계산 후 투영\n",
        "print('선택한 차원(픽셀) 수 :', pca.n_components_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw3oLq-ckGY2"
      },
      "source": [
        "## Scikit learn으로 간단하게 구하기\n",
        "# 적절한 차원의 개수 구하기\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(X)\n",
        "\n",
        "print('singular value :', pca.singular_values_)\n",
        "print('singular vector :\\n', pca.components_.T)\n",
        "\n",
        "print('eigen_value :', pca.explained_variance_)\n",
        "print('explained variance ratio :', pca.explained_variance_ratio_)\n",
        "\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1\n",
        "print('선택할 차원 수 :', d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPgBWpc4kGfY"
      },
      "source": [
        "## 다양한 차원축소법들 예제\n",
        "# KPCA 예제\n",
        "from sklearn.datasets import make_swiss_roll\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42) \n",
        "\n",
        "# KernelPCA 예제\n",
        "from sklearn.decomposition import KernelPCA\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "\n",
        "# MDS 예제\n",
        "from sklearn.manifold import MDS\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "X_reduced_mds = mds.fit_transform(X)\n",
        "\n",
        "# Isomap 예제\n",
        "from sklearn.manifold import Isomap\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduced_isomap = isomap.fit_transform(X)\n",
        "\n",
        "# TSNE 예제\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_reduced_tsne = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJIHkAqKdtEs"
      },
      "source": [
        "## 🔎 T-sne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvaMWOh6ef4r"
      },
      "source": [
        "- T-sne는 데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾기 위한 차원축소 기법이다.\n",
        "- 차원을 3차원으로밖에 축소하지 못해서 모델 학습에는 적합하지는 않다.\n",
        "- 대신 2차원으로 시각화해서 데이터의 분포 및 특성 파악, Outlier 파악에 유용하다.\n",
        "- 밑에 사진은 숫자 손글씨 데이터셋을 PCA와 T-sne로 차원축소한 뒤 class별로 Scatter plot을 그린 결과이다.\n",
        "\n",
        "> PCA로 차원축소한 결과\n",
        "![](https://t1.daumcdn.net/cfile/tistory/99C3C7455AB1FA1E22)\n",
        "\n",
        "> T-sne로 차원축소한 결과\n",
        "![](https://t1.daumcdn.net/cfile/tistory/997118395AB1FA2103)\n",
        "\n",
        "- 데이터를 EDA하는 과정에서는 T-sne가 시각화하기에 적합하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsda25F5fVaX"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lfclpzrd98u"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "df_sort_not_zero = df_sort[df_sort['MRC_ID_DI'] != 0]\n",
        "\n",
        "df_sort_values = df_sort_not_zero.iloc[:,30:]\n",
        "\n",
        "distance_matrix_doc = pairwise_distances(df_sort_values, df_sort_values, metric='cosine', n_jobs=-1)\n",
        "tsne_doc = TSNE(metric=\"precomputed\", n_components=2, verbose=1, n_iter=500)\n",
        "tsne_results_doc = tsne_doc.fit_transform(distance_matrix_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0DILaYQfYzy"
      },
      "source": [
        "## 🔎 Factor Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1Q4tYG3yEJ"
      },
      "source": [
        "- [Reference](https://www.datacamp.com/community/tutorials/introduction-factor-analysis)\n",
        "- PCA가 variance를 설명하는 거라면, Factor analysis는 covariance를 설명하는 방법이다.\n",
        "- Factor는 여러 개의 column의 패턴을 종합한 요소로 적어진 칼럼을 바탕으로 데이터를 설명할 수 있게 해준다.\n",
        "- 각 factor들은 variables의 일정 부분의 variance를 설명한다.\n",
        "<br>\n",
        "\n",
        "<h3> 📔 Assumption </h3>\n",
        "\n",
        "- 데이터에 outlier가 없다.\n",
        "- sample이 factor 보다 더 많다.\n",
        "- 다중공선성이 없다.\n",
        "- 변수간에 등분산이 없어야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5buyGvQfyMZ"
      },
      "source": [
        "<h3> 📔 Terminology </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-4nkO0fyJA"
      },
      "source": [
        "- Factor loading: A matrix which shows the relationship of each variable to the underlying factor. It shows the correlation coefficient for observed variable and factor. It shows the variance explained by the observed variables.\n",
        "- Eigenvalues: 전체 분산에서 factor가 설명하는 분산의 크기를 나타냄\n",
        "- Communalities: Sum of the squared loadings for each variable. It represents the common variance\n",
        "- Factor Rotation: Rotation is a tool for better interpretation of factor analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwO9BKVcfyF9"
      },
      "source": [
        "<h3> 📔 PCA vs FA </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GzBTbiEfyC5"
      },
      "source": [
        "- PCA가 variance를 설명하는 거라면, Factor analysis는 covariance를 설명하는 방법이다.\n",
        "- PCA components are fully orthogonal to each other $<->$ FA is not\n",
        "- PCA component is a linear combination of the observed variable $<->$ FA, the observed variables are linear combinations of the unobserved variable or factor.\n",
        "- PCA는 해석 불가능 $<->$ FA는 해석가능\n",
        "- PCA is a kind of dimensionality reduction method $<->$ FA is the latent variable method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH-1CFGmfyAG"
      },
      "source": [
        "<h3> 📔  Adequacy Test </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7r_bpj7f3Qq"
      },
      "source": [
        "- FA를 하기 전에 Factorability를 측정해야 한다.\n",
        "- Factorability는 \"Can we find the factors in the dataset?\" 질문에 대한 대답이다.\n",
        "- 방법으로는 두 가지를 사용할 수 있다.\n",
        "  - Bartlett's Test\n",
        "  - Kaiser-Meyer-Olkin Test\n",
        "\n",
        "> Bartlett's Test\n",
        "- Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. - If the test found statistically insignificant, you should not employ a factor analysis.\n",
        "- If test is statistically significant, it indicates that the observed correlation matrix is not an identity matrix.\n",
        "``` python\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
        "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
        "chi_square_value, p_value\n",
        "```\n",
        "\n",
        "> Kaiser-Meyer-Olkin (KMO) Test\n",
        "- Measures the suitability of data for factor analysis.\n",
        "- KMO estimates the proportion of variance among all the observed variable.\n",
        "```python\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "kmo_all,kmo_model=calculate_kmo(df)\n",
        "kmo_model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wwmc_qnfk9A"
      },
      "source": [
        "<h3> 📔 Types of PCA </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fQL8KKftaM"
      },
      "source": [
        "<h3> 🎈Exploratory Factor Analysis</h3>\n",
        "\n",
        "- 가장 유명. any variable이 any factor와 연결이 되어있다는 가정을 함\n",
        "\n",
        "<h3> 🎈Confirmatory Factor Analysis </h3>\n",
        "\n",
        "- factor가 variable에 연결되있다는게 가정.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amHFx7ymf633"
      },
      "source": [
        "### 📔 Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq9S2Qeq_A8r"
      },
      "source": [
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
        "chi_square_value,p_value = calculate_bartlett_sphericity(ans)\n",
        "chi_square_value, p_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWdyXiKeRld3"
      },
      "source": [
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "kmo_all,kmo_model=calculate_kmo(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuhF0fGZ32BV"
      },
      "source": [
        "# Factor analysis 예시\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "fa = FactorAnalyzer(rotation='varimax',  n_factors = 17)\n",
        "fa.fit(df)\n",
        "\n",
        "# 스크리 도표 그리기\n",
        "ev, v = fa.get_eigenvalues()\n",
        "\n",
        "plt.scatter(range(1,df.shape[1]+1),ev)\n",
        "plt.plot(range(1,df.shape[1]+1),ev)\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Factors')\n",
        "plt.ylabel('Eigenvalue')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Get explanined varaince by factor.\n",
        "# SS Loadings, Proportion Var, Cumulative Var\n",
        "fa.get_factor_variance()\n",
        "\n",
        "# column별 factor의 영향 확인하기\n",
        "pd.DataFrame(fa.loadings_)\n",
        "\n",
        "# fa 적용하여 새로운 column에 맞게 변형하기\n",
        "fa.transform(df.values)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}