{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(원본) [ML Intro] Maching Learning Codebook",
      "provenance": [],
      "collapsed_sections": [
        "sw5d6I-YbImk",
        "Q5DgX7QHcx91",
        "zNswN79X-4XH",
        "0hZLHHGP-4XI",
        "Eu9wklSp-4XJ",
        "eG8NJqwv-4XK",
        "E-zlueslZ7fe",
        "DSRYORPDZ7uv",
        "6aGwgB_wbCrO",
        "s61epZtz_JRI",
        "11nm8K92_JRV",
        "kkchB18abGyc",
        "uHVla3OfbG2m",
        "jiAATuLcbG-b",
        "61q0ZkG2bHJx",
        "-1SG_RNbIx5h",
        "TlmT50pyI2Df",
        "lL1W4FCFbND6",
        "dXxqASE8bNTj",
        "aBQ_pbhmbTXV",
        "bJWmUSDlAyOK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYFuejL5sXMt"
      },
      "source": [
        "---\n",
        "# 📁 Hyun's Code collection (Machine Learning Intro) \n",
        "---\n",
        "\n",
        "- What is Machine Learning?\n",
        "- Essential Concepts\n",
        "- Types of Machine Learning\n",
        "- Overall Process of Machine Learning Project\n",
        "- Extra Tips\n",
        "\n",
        "### <h3 align=\"right\">🥇 Authored by <strong>Hyun</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw5d6I-YbImk"
      },
      "source": [
        "# ✏️ What is **Machine Learning**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpmsSftm2QQq"
      },
      "source": [
        "![](https://lh3.googleusercontent.com/HK5QOxzeXCF0CMWSTae2OkxXmxETt0lNlyTYrYhVTiNrGxFr33lLgCnjfGiSSQgVVmJhjlkx4gM8eo71BBV6HW_IAo1b-kzqJL4ZLcqeiUhXIYin0vjeTjxrJOQWRmNSve6TX-er2RQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2wWk6km_CWp"
      },
      "source": [
        "- 머신러닝은 간단하게 말해서 데이터로부터 학습할 수 있는 시스템을 만드는 것\n",
        "- 학습이란? 어떤 작업에서 주어진 성능 지표가 더 나아지는 것을 의미함!\n",
        "\n",
        "- 기존 **데이터를 바탕**으로 **기계가 학습**하는 것을 의미\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5DgX7QHcx91"
      },
      "source": [
        "# ✏️ Essential Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNswN79X-4XH"
      },
      "source": [
        "## 🔎 Bias vs Variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr8EG5Vb-4XI"
      },
      "source": [
        "\n",
        "\n",
        "![bias](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F99CDCC33599AC28F075E3C)\n",
        "\n",
        "> Bias란?\n",
        " - Bias는 실제 값에서 멀어진 척도 \n",
        "\n",
        "> Variance란?\n",
        " - Variance는 예측된 값들이 서로 얼마나 멀리 떨어져 있는지에 대한 척도\n",
        "\n",
        "> Machine Learning Model의 목표\n",
        " - 왼쪽 위의 그림처럼 Bias도 낮고, Variance도 낮게, 즉 모두 정확하게 예측하는 것!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hZLHHGP-4XI"
      },
      "source": [
        "## 🔎 Cost function, Loss function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjWMlkN-4XI"
      },
      "source": [
        "> Loss function\n",
        "- Loss function은 data point에서의 오차\n",
        "- Single data set에서의 오차를 구하는 것\n",
        "순간순간의 loss를 판단할 땐 loss function  \n",
        "  학습이 완료된 후에는 cost function!\n",
        "\n",
        "\n",
        "> Cost function\n",
        "- Cost function은 loss function의 합이다. \n",
        "- 즉 entire data set에서 loss funciton을 계산한 합이다.\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu9wklSp-4XJ"
      },
      "source": [
        "## 🔎 Overfitting, Underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyLAT0EN-4XJ"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "<br>\n",
        "\n",
        "![123123](https://mblogthumb-phinf.pstatic.net/MjAxODA3MzBfMjMy/MDAxNTMyODkwNjUxMjY4.H_ocFIRFaG8MWrBsv8BWrTCaAMGLMKZZUh_Rd1krRLog.HAZRdDtrQMvVGKiEWfGls8bm0EhTyRKf7XzoSY1Cibsg.JPEG.qbxlvnf11/maxresdefault.jpg?type=w800)\n",
        "\n",
        "> **과대적합(Overfitting)**이란?\n",
        " - 모델이 Train Dataset에 너무 잘 맞아서 일반성이 떨어지게 되는 문제\n",
        " - 즉 Train Dataset을 너무 과하게 학습해 학습되지 않은 데이터가 들어오면 분류하지 못하게 되는 것\n",
        " - 위 그림의 오른쪽 그림들이 과대적합의 예시\n",
        " - 보시다시피 Train Set을 거의 다 거치거나 분류해내며 굉장히 높은 성능을 보여주고 있지만, 새로운 변수에 대응하기 어려움\n",
        " - 그림의 가운데 모델이 적합한 모델이라고 할 수 있음!\n",
        "\n",
        "> **과소적합(Underfitting)**이란?\n",
        " - Overfitting과 반대 개념\n",
        " - 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못하는 것\n",
        " - 위 그림의 왼쪽 그림이 underfitting.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMvaT_ZRiXGA"
      },
      "source": [
        "\n",
        "\n",
        "> Machine Learning에서 **Overfitting을 방지하는 방법**\n",
        " - 더 많은 데이터로 학습하기\n",
        " - Parameter가 적은 모델을 선택\n",
        " - Feature 개수 줄이기\n",
        " - 정규화(Regularization) 시키기 (e.g. L1, L2)  \n",
        "   ```1.2 Machine Learning 관련 용어모음``` 확인\n",
        " - Early stopping: 어느 정도 loss가 줄어들지 않으면 학습 멈추기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji66dr3tiqQj"
      },
      "source": [
        "> Bias, Variance and fitting\n",
        "1. Overfitting 모델\n",
        " - High Variace 모델(오른쪽 위)\n",
        " - Train Data의 지엽적인 특성까지 반영되어 학습된 것은 잘 예측하지만, 학습되지 않은 데이터에 대해서는 예측력이 떨어지게 됨\n",
        "2. Underfitting 모델\n",
        " - High Bias 모델(왼쪽 아래)\n",
        " - 너무 적은 특성만을 반영하여, 예측의 범위가 좁고 정확도가 떨어지게 됨\n",
        "\n",
        "- 다시 말하면, Bias 에러가 높아지는 것은 많은 데이터를 고려하지 않아(=모델이 너무 단순해) 정확한 예측을 하지 못하는 경우를 말하고, Variance(분산) 에러는 노이즈까지 전부 학습해(=모델이 너무 복잡해) 약간의 input에도 예측 Y 값이 크게 흔들리는 것을 말함. \n",
        "- 이 두가지 에러가 **Trade-off** 관계에 있어서 이 둘을 모두 잡는 것은 불가능 한 딜레마가 생김\n",
        " ![graph](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile9.uf.tistory.com%2Fimage%2F996DB433599AC34225B9BD)\n",
        "- Total Error가 최소인 지점을 찾기 위한 가장 효과적인 방법은 **Validation Set을 만드는 것**이다.  \n",
        "(밑에서 Validation Set 부가설명)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG8NJqwv-4XK"
      },
      "source": [
        "## 🔎 Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlGi0qnH-4XM"
      },
      "source": [
        "<h2> 🎈 Accuracy (정확도) </h2>\n",
        "\n",
        "\n",
        "- 정확도(accuracy)란 전체 샘플 중 맞게 예측한 샘플 수의 비율\n",
        "- 높을수록 좋은 모형이고 일반적으로 학습에서 최적화 목적함수로 사용됨!!\n",
        "- 정확도(Accuracy)의 가장 큰 문제점은 **클래스의 분포가 같을 때만 이용 가능하다는 점**\n",
        "- 이러한 **단점을 보완**하는 지표가 정밀도(Precision)와 재현율(Recall), ROC 곡선과 AUC입니다.\n",
        "- $$accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$<br>\n",
        "``` python\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "accuracy_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> 🎈 Attribute (속성) </h2>\n",
        "\n",
        "\n",
        "- 데이터 타입을 의미\n",
        "- Feature(특성)은 attribute + 값. 허나 보통 많은 사람이 feature와 attribute를 구분하지 않고 사용\n",
        "\n",
        "<h2> 🎈 AUC </h2>\n",
        "\n",
        "- AUC란 ROC 곡선 아래 부분의 면적이다. 1에 가까울수록 성능이 좋음!\n",
        "\n",
        "<h2> 🎈 Batch Learning <-> Online learning </h2>\n",
        "\n",
        "- 배치 학습!\n",
        "- 가용한 데이터를 모두 사용해 훈련시켜야 함\n",
        "- 시간과 자원을 많이 소모\n",
        "- 보통 오프라인에서 수행\n",
        "- 시스템이 훈련되고 적용되면 더 이상의 학습없이 실행된다.\n",
        "\n",
        "<h2> 🎈 Batch size </h2>\n",
        "\n",
        "- Batch size: data를 나누는 것\n",
        "\n",
        "<h2> 🎈 Bagging </h2>\n",
        "\n",
        "- Bagging은 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과를 집계(Aggregating) 하는 방법이다.\n",
        "\n",
        "<h2> 🎈 Bias </h2>\n",
        "\n",
        " - Bias는 실제 값에서 멀어진 척도 \n",
        "\n",
        "<h2> 🎈 Bootstrapping </h2>\n",
        "\n",
        " - 통계학에서 가설을 test하거나 metric을 계산하지 전에 '중복을 허용한' random sampling을 적용하는 방법\n",
        " - Machine learning에서 bootstrapping이란 random sampling을 통해 train data를 늘리는 방법이다!\n",
        " - Overfitting을 줄이는 데 도움이 된다.\n",
        "\n",
        "<h2> 🎈 Cross validation </h2>\n",
        "\n",
        "- Cross validation은 validation set을 뗄 만큼 data가 크지 않을 때 사용\n",
        "- Training Set을 여러 Subset으로 나누고 각 모델을 이 Subset의 조합으로 훈련시키고 나머지 부분으로 검증하는 방법이다.\n",
        "\n",
        "<h2> 🎈 Cost function </h2>\n",
        "\n",
        "- Cost function은 loss function의 합이다. \n",
        "- 모델이 얼마나 나쁜지 측정하는 함수 <-> 효용함수\n",
        "- 즉 entire data set에서 loss funciton을 계산한 합이다.\n",
        "- 순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function is a part of a cost function which is a type of an objective function!!!\n",
        "\n",
        "<h2> 🎈 Entropy </h2>\n",
        "\n",
        "- 불확실성에 대한 척도.\n",
        "- Entropy 함수  \n",
        "  $H_p(X) = \\mathbb{E}\\big[I(X)\\big] = \\mathbb{E} \\big[ \\log (\\frac{1}{p(X)}) \\big] = -\\sum_{i=1}^{n} p(x_i)\\log(p(x_i))$  \n",
        "  C는 범주의 갯수, q는 사건의 확률질량함수(probability mass function)\n",
        "- 예측이 어려울수록 정보의 양은 더 많아지고 엔트로피는 커진다.\n",
        "- 확률적으로 발생하는 사건에 대한 **정보량의 평균**. 놀람의 정도를 나타낸다고 볼 수 있다.\n",
        "\n",
        "<h2> 🎈 Estimator (추정기) </h2>\n",
        "\n",
        "- dataset을 기반으로 일련의 모델 parameter를 추정하는 객체\n",
        "- 추정은 fit( ) 메서드에 의해 수행되고 하나의 매개변수로 하나의 dataset만 전달\n",
        "\n",
        "<h2> 🎈 Epoch </h2>\n",
        "\n",
        "- one epoch: 모든 training example을 도는 것!\n",
        "\n",
        "<h2> 🎈 Feature </h2>\n",
        "\n",
        "- attribute(속성)와 값이 합쳐진 형태\n",
        "- e.g 주행거리 = 15,000\n",
        "\n",
        "<h2> 🎈 Gini Index </h2>\n",
        "\n",
        "- $G.I(A)=\\sum _{ i=1 }^{ d }{ { \\left( { R }_{ i }\\left( 1-\\sum _{ k=1 }^{ m }{ { p }_{ ik }^{ 2 } }  \\right)  \\right)  } }$\n",
        "- 정보의 순도를 측정할 때 사용하는 지표 중 하나\n",
        "\n",
        "<h2> 🎈 Gradient </h2>\n",
        "\n",
        "- Parameter들의 편미분계수, 기울기\n",
        "\n",
        "<h2> 🎈 Hold-out Validation </h2>\n",
        "\n",
        "- Validation set을 만드는 방법 중 하나\n",
        "- 데이터를 무작위로 train set과 validation set, test set으로 구분한 뒤, train set과 validation set을 이용해 분석 모형을 구축하고, test set을 이용하여 분석 모형의 성능을 평가하는 방법 \n",
        "- step.1\n",
        " - Original Set을 무작위로 train set과 validation set, test set으로 구분!\n",
        "- step.2\n",
        "  - hyperparameter를 다르게 해 여러가지 세팅을 만들어본 뒤, 학습 알고리즘을 사용해 Train set에 모델을 학습시키뮤\n",
        "- step.3\n",
        "  - Validation Set로 모델의 성능을 평가해 가장 좋은 성능의 hyperparameter 세팅을 선택함\n",
        "- step.4\n",
        "  - Train Set은 보통 클수록 좋음. 그러므로 모델 선택 후에 Train Set와 Validation Set을 합쳐 더 큰 데이터셋으로 step.3에서 선택한 최선의 hyperparameter 세팅을 사용한 모델을 학습하기!\n",
        "- step.5 \n",
        "  - 이제 Test Set을 사용해 모델의 성능을 평가~\n",
        "- Cross Validation과의 차이점: Hold-out Validation에서 Test Set의 경우 모형에는 영향을 주지 않고, 모델의 성능 측정 만을 위해 사용된다는 점! \n",
        "\n",
        "<h2> 🎈 Hyperparameter </h2>\n",
        "\n",
        "- 학습의 대상이 아니라 학습 이전에 정해놓은 변수, 학습 알고리즘의 parameter\n",
        "- Parameter는 학습의 대상이 되는 변수\n",
        "\n",
        "<h2> 🎈 Iteration </h2>\n",
        "\n",
        "- batch를 몇 번 학습에 사용했냐?\n",
        "- e.g. 1000개의 training set이 있고, batch size는 500이다. 그러므로 1 epoch 도는 동안 2 iteration이다.\n",
        "\n",
        "<h2> 🎈 Learning rate </h2>\n",
        "\n",
        "- 계산한 기울기에 비례하여 parameter를 얼마만큼 업데이트할지 결정하는 수치\n",
        "- 배우는 속도. 데이터와 모델에 따라서 최적값은 모두 다르다.\n",
        "- 보통 실무에서는 초기에 비교적 높은 lr로 시작하여 점차 낮추는 전략을 취함. 단 오히려 batch size를 늘리는 게 더 좋다는 연구도 있음\n",
        "- Learning rate가 너무 크면?? \n",
        "  - 발산해버리게 된다. Cost가 무진장 늘어난다.\n",
        "  - 데이터에 빨리 적응하지만 이전 데이터를 금방 까먹음\n",
        "- Learning rate가 너무 작으면? \n",
        "  - cost 값이 변함이 없는 것을 알 수 있다.\n",
        "  - 관성이 커져서 느리게 학습됨\n",
        "\n",
        "<h2> 🎈 Learning rate schedule </h2>\n",
        "\n",
        "- 매 iteration에서 learning rate을 바꿔주는 함수\n",
        "\n",
        "<h2> 🎈 Logit </h2>\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Logit.svg/350px-Logit.svg.png)\n",
        "- [Logit, Sigmoid, Softmax의 관계](https://opentutorials.org/module/3653/22995)\n",
        "\n",
        "<h2> 🎈 Loss function   </h2>\n",
        "\n",
        "- Loss function은 data point에서의 오차. Single data set에서의 오차를 구하는 것\n",
        "순간순간의 loss를 판단할 땐 loss function, 학습이 완료된 후에는 cost function!\n",
        "- A loss function is a part of a cost function which is a type of an objective function!!!\n",
        "\n",
        "<h2> 🎈 MAE (Mean Absolute Error) </h2>\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "<h2> 🎈 MAPE (Mean Asolute Percentage Error) </h2>\n",
        "\n",
        "- Scale Dependent Error의 단점을 보완하기 위한 방법이다.\n",
        "- 하지만 MAPE 역시 실제 예측 값이 1보다 작을 경우 분모가 작아져 무한대에 가까워질 수 있다는 단점이 있다.\n",
        "``` python\n",
        "from sklearn.utils import check_arrays\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = check_arrays(y_true, y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "```\n",
        "\n",
        "<h2> 🎈 MSE(Mean Squared Error) </h2>\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "<h2> 🎈 Objective function </h2>\n",
        "\n",
        "- The most general term for any function that you optimize during training.\n",
        "- For example, a probability of generating training set in maximum likelihood approach is a well defined objective function, but it is not a loss function nor cost function (however you could define an equivalent cost function). For example:\n",
        "\n",
        "<h2> 🎈 Online learning <-> Batch learning </h2>\n",
        "\n",
        "- 데이터를 순차적으로 한 개씩 또는 mini-batch 단위로 주입해서 시스템을 훈련시킴\n",
        "- 새로운 데이터가 도착하는 대로 즉시 학습할 수 있다.\n",
        "\n",
        "\n",
        "<h2> 🎈 Overfitting </h2>\n",
        "\n",
        " - 모델이 Train Dataset에 너무 잘 맞아서 일반성이 떨어지게 되는 문제\n",
        " - 즉 Train Dataset을 너무 과하게 학습해 학습되지 않은 데이터가 들어오면 분류하지 못하게 되는 것\n",
        " - 위 그림의 오른쪽 그림들이 과대적합의 예시\n",
        " - 보시다시피 Train Set을 거의 다 거치거나 분류해내며 굉장히 높은 성능을 보여주고 있지만, 새로운 변수에 대응하기 어려움\n",
        " - 그림의 가운데 모델이 적합한 모델이라고 할 수 있음!\n",
        "\n",
        "<h2> 🎈 OvA 전략 </h2>\n",
        "\n",
        "- label 개수만큼 분류기를 만드는 방법\n",
        "\n",
        "<h2> 🎈 OvO 전략 </h2>\n",
        "\n",
        "- label combination 2만큼의 조합마다 만들 수 있는 분류기를 다 만드는 방법\n",
        "\n",
        "<h2> 🎈 Partial Derivative </h2>\n",
        "\n",
        "- 편도함수\n",
        "- 모델 parameter에 대해 비용 함수 gradient를 계산하는 것!\n",
        "- 즉, parameter가 조금 변경될 때 cost function이 얼마나 바뀐지 계산하는 함수\n",
        "\n",
        "<h2> 🎈 Precision (정밀도) </h2>\n",
        "\n",
        "- 정밀도(precision)란 positive 클래스에 속한다고 분류한 샘플 중 실제로 positive 클래스에 속하는 샘플 수의 비율\n",
        "- 높을수록 좋은 모형!!!\n",
        "$$precision=\\frac{TP}{TP+FP}$$\n",
        "``` python\n",
        "from sklearn.metrics import precision_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "precision_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> 🎈 Recall (재현율) </h2>\n",
        "\n",
        "- 재현율(recall)이란 실제 positive 클래스에 속한 표본 중에 positive 클래스에 속한다고 출력한 표본의 수의 비율\n",
        "- 높을수록 좋은 모형!!\n",
        "- TPR(true positive rate) 또는 민감도(sensitivity)라고도 합니다.\n",
        "$$recall=\\frac{TP}{TP+FN}$$\n",
        "``` python\n",
        "from sklearn.metrics import recall_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "recall_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "<h2> 🎈 ROC </h2>\n",
        "\n",
        "- ROC이란 TPR과 FPR은 어떤 기준을 연속적으로 바꾸며 측정해야 하는데 이를 한눈에 볼 수 있게 한 것\n",
        "- ![ROC_curve](https://t1.daumcdn.net/cfile/tistory/262E8E3F544837AD27)<br>\n",
        "- 위 그림처럼 ROC 곡선은 TPR과 FPR이 둘다 [0,1]의 범위이며 (0,0)에서 (1,1)을 잇는 곡선이다. \n",
        "\n",
        "<h2> 🎈 Sampling Noise </h2>\n",
        "\n",
        "- 샘플이 작을 때 우연에 의한 대표성이 없는 데이터가 생겼음을 의미\n",
        "\n",
        "<h2> 🎈 Sigmoid function </h2>\n",
        "\n",
        "- $S(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "<h2> 🎈 Transformer(변환기) </h2>\n",
        "\n",
        "- dataset을 변환하는 estimator를 transformer라고 함\n",
        "\n",
        "<h2> 🎈 Underfitting </h2>\n",
        "\n",
        " - Overfitting과 반대 개념\n",
        " - 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못하는 것\n",
        " - 위 그림의 왼쪽 그림이 underfitting.\n",
        "\n",
        "<h2> 🎈 Variance </h2>\n",
        "\n",
        " - Variance는 예측된 값들이 서로 얼마나 멀리 떨어져 있는지에 대한 척도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-zlueslZ7fe"
      },
      "source": [
        "# ✏️ Types of Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IknJz98B6eFD"
      },
      "source": [
        "![123123](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0goZ-1yFjQ6l"
      },
      "source": [
        "- Machine Learning에는 크게 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning)으로 나뉜다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7DZyJVcHzi"
      },
      "source": [
        "\n",
        "> 지도학습\n",
        "- 답이 이미 있는 데이터를 기계가 학습한 뒤, 새로운 데이터가 들어왔을 때 답을 예측하는 방법\n",
        "- Regression, Classification이 이에 해당\n",
        "\n",
        "> 비지도학습\n",
        " - 답이 없는 데이터의 특징들을 기계가 스스로 학습한 뒤 이를 바탕으로 답을 내주는 방법\n",
        " - Clustering, Dimensionality Reduction, Visualization, 연관규칙학습이 이에 해당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKvXhJJYZ7nq"
      },
      "source": [
        "# ✏️ Overall Process of Machine Learning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSRYORPDZ7uv"
      },
      "source": [
        "## 🔎 1. Set Project's Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5sW7b_7ah0T"
      },
      "source": [
        "- 우리가 해결하고자 하는 문제가 뭔지?\n",
        "- Machine Learning 모델을 통해서 해결하고자 하는 문제를 설정!\n",
        "\n",
        "> 예시\n",
        " - 매출 회복을 위해 고객별 STP 전략을 활용한 타겟 마케팅 전략 수립\n",
        " - 사기거래의 비중이 높아지는데 사기거래를 예측을 통한 비용 감소\n",
        " - 고객들이 주문할 상품량을 예측을 통한 물류 시스템 효율성 증진\n",
        " - 어린아이에게 안전한 동영상을 걸러내주는 모델 만들기\n",
        " - CCTV로 좀도둑을 잡는 모델 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aGwgB_wbCrO"
      },
      "source": [
        "## 🔎 2. Select Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s61epZtz_JRI"
      },
      "source": [
        "### 🎈 Regression Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khIL1lzv_JRJ"
      },
      "source": [
        "- Regression은 평가할 때 **실제 답과 예측값**이 얼마나 떨어져있는지로 평가!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TEuHGJ5_JRJ"
      },
      "source": [
        "- MAE, MSE, RMSE 모두 vector와 target vector 사이의 거리를 재는 방법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4LoIqkmnNm"
      },
      "source": [
        "> 📎 MAE(Mean Absolute Error)\n",
        "- $MAE = \\frac{1}{N}\\sum_{(i=1)}^N\\left\\lvert{f_i-y_i}\\right\\rvert$\n",
        " ```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "> 📎 MSE(Mean Squared Error)\n",
        "- $MSE = \\frac{1}{N}\\sum_{(i=1)}^N(f_i-y_i)^2$\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(y_test, y_predict)\n",
        "```\n",
        "\n",
        "> 📎 RMSE(Root Mean Squared Error)  \n",
        "- $RMSE = \\sqrt{\\frac{1}{N}\\sum_{(i=1)}^N(f_i-y_i)^2}$\n",
        "- RMSE가 MAE보다 조금 더 outlier에 민감함\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse\n",
        "```\n",
        "\n",
        "> 📎 MAPE  \n",
        "- $MAPE = \\frac{1}{n}\\sum_{t=1}^n |\\frac{y_i-f_i}{y_i}|$\n",
        "- Scale Dependent Error의 단점을 보완하기 위한 방법이다.\n",
        "- 하지만 MAPE 역시 실제 예측 값이 1보다 작을 경우 분모가 작아져 무한대에 가까워질 수 있다는 단점이 있다.\n",
        "```python\n",
        "from sklearn.utils import check_arrays\n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "    y_true, y_pred = check_arrays(y_true, y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11nm8K92_JRV"
      },
      "source": [
        "### 🎈 Classification Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7zE1spi_JRW"
      },
      "source": [
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/99DC064C5BE056CE10\" alt=\"Drawing\" style=\"width: 100px;\"/>  \n",
        "- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)\n",
        "- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)\n",
        "- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)\n",
        "- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)<br>\n",
        "[reference](https://frhyme.github.io/machine-learning/clf_%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0/)  \n",
        "\n",
        "\n",
        "> <h3> 📎 정확도(accuracy) </h3>\n",
        "\n",
        "- 정확도(accuracy)란 전체 샘플 중 맞게 예측한 샘플 수의 비율\n",
        "- 높을수록 좋은 모형이고 일반적으로 학습에서 최적화 목적함수로 사용됨!!\n",
        "- 정확도(Accuracy)의 가장 큰 문제점은 **클래스의 분포가 같을 때만 이용 가능하다는 점**\n",
        "- 이러한 **단점을 보완**하는 지표가 정밀도(Precision)와 재현율(Recall), ROC 곡선과 AUC입니다.\n",
        "$$accuracy=\\frac{TP+TN}{TP+TN+FP+FN}$$<br>\n",
        "``` python\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "accuracy_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> 📎 정밀도(precision) </h3>\n",
        "\n",
        "- 정밀도(precision)란 positive 클래스에 속한다고 분류한 샘플 중 실제로 positive 클래스에 속하는 샘플 수의 비율\n",
        "- 높이려면 진짜진짜 아주아주 확실한 것 하나만 제대로 예측하면 완벽한 정밀도를 얻음!\n",
        "- 높을수록 좋은 모형!!!\n",
        "$$precision=\\frac{TP}{TP+FP}$$\n",
        "``` python\n",
        "from sklearn.metrics import precision_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "precision_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> 📎 재현율(recall) </h3>\n",
        "\n",
        "- 재현율(recall)이란 실제 positive 클래스에 속한 표본 중에 positive 클래스에 속한다고 출력한 표본의 수의 비율\n",
        "- 높을수록 좋은 모형!!\n",
        "- TPR(true positive rate) 또는 민감도(sensitivity)라고도 합니다.\n",
        "$$recall=\\frac{TP}{TP+FN}$$\n",
        "``` python\n",
        "from sklearn.metrics import recall_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "recall_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "> <h3> 📎 F Score, F1 score </h3>\n",
        "\n",
        "- F Score는 precision과 recall의 **가중조화평균**\n",
        "- 베타($\\beta$)는 정밀도에 주어지는 가중치\n",
        "$$F_β=\\frac{(1+β^2)(precision*recall)}{β^2precision+recall}$$  \n",
        "- 베타가 1인 경우가 F1 Score!\n",
        "$$F_1=\\frac{2⋅precision⋅recall}{precision+recall}$$\n",
        "``` python\n",
        "from sklearn.metrics import f1_score\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "f1_score(y_true, y_pred)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKTUvk_L7wT"
      },
      "source": [
        "> <h3> 📎 Precision vs Recall vs F1 score </h3>\n",
        "\n",
        "- 무조건 F1 score만 보는 것은 잘못 됨!\n",
        "- 맨 처음 큰 그림을 볼 때 목표/문제 정의에 따라 봐야되는 성능지표가 달라진다!\n",
        "- 유해동영상을 어린아이에게 걸러내는 분류기를 훈련시킨다면 좋은 동영상이 많이 제외되더라도(낮은 재현율) 안전한 것들만 노출시키는(높은 정밀도) 모델이 좋음!\n",
        "- 좀도둑을 잡는 분류기라면 경비원이 잘못된 호출로 고생하셔도(정확도가 30%라서) 재현율이 99%라면 모든 좀도둑을 잡은 것이기 때문에 좋은 모델! \n",
        "- 분류기에서 predict( ) 메서드 대신 decision_function( ) 메서드를 쓰면 [샘플의 점수]를 얻을 수 있다.\n",
        "- 이 점수를 기반으로 threshold를 정해 예측을 만들 수 있다.\n",
        "- precision_recall_cureve( ) 함수를 이용해서 모든 threshold에 대해 recall과 precision을 계산가능\n",
        "```python\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
        "```\n",
        "- 그래프 그리기\n",
        "```python\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"정밀도\", linewidth=2)\n",
        "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"재현율\", linewidth=2)\n",
        "    plt.xlabel(\"임계값\", fontsize=16)\n",
        "    plt.legend(loc=\"upper left\", fontsize=16)\n",
        "    plt.ylim([0, 1])\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "plt.xlim([-700000, 700000])\n",
        "save_fig(\"precision_recall_vs_threshold_plot\")\n",
        "plt.show()\n",
        "```\n",
        "![](data:image/png;base64,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)\n",
        "\n",
        "- 작업에 맞는 최선의 threshold를 선택하면 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-XhT0BCL-WQ"
      },
      "source": [
        "> <h3> 📎 Classification_report </h3>\n",
        "\n",
        "- Classification report는 sklearn 패키지의 metrics 패키지에서 precision, recall , F1 score를 구할 때 쓰는 메소드\n",
        "- Classification report는 각각의 클래스를 양성(positive) 클래스로 보았을 때의  precision, recall , F1 score를 각각 구하고 그 평균값으로 전체 모형의 성능을 평가함!!\n",
        "\n",
        "``` python\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
        "y_pred = [0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=['class 0', 'class 1']))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_doQAvvMAh4"
      },
      "source": [
        "> <h3> 📎 ROC, AUC </h3>\n",
        "\n",
        "- ROC이란 TPR과 FPR은 어떤 기준을 연속적으로 바꾸며 측정해야 하는데 이를 한눈에 볼 수 있게 한 것\n",
        "- AUC란 ROC 곡선 아래 부분의 면적이다. 1에 가까울수록 성능이 좋음!\n",
        "- ![ROC_curve](https://t1.daumcdn.net/cfile/tistory/262E8E3F544837AD27)<br>\n",
        "- 위 그림처럼 ROC 곡선은 TPR과 FPR이 둘다 [0,1]의 범위이며 (0,0)에서 (1,1)을 잇는 곡선이다. \n",
        "\n",
        "- TRP(True Positive Rate) = recall\n",
        "$$TPR=\\frac{TP}{TP+FN}$$<br>\n",
        "- FPR(False Positive Rate)\n",
        "$$FPR=\\frac{FP}{FP+TN}$$<br>\n",
        "- TPR과 FPR은 반비례 관계에 있음\n",
        "  - 예시) 3인지 아닌지 판단할 때, 조금만 3처럼 보여도 모두 3이라고 분류할 경우, 이 때의 TPR은 1에 가까워지지만 반대로 FPR은 매우 낮아진다. 반대로 조금만 비슷하지 않아도 모두 3이 아니라고 분류할 경우 TPR은 급격히 낮아져 0에 가까워지겠지만, FPR은 반대로 급격히 높아져 1에 가까워진다.(3이라고 분류 자체를 안하므로, 잘못 분류하는 경우가 없는 것). 이처럼 TPR과 FPR은 어떤 기준(언제 3이라고 예측할까?)을 연속적으로 바꾸며 측정해야 하는데 이를 한눈에 볼 수 있게 한 것이 바로 **ROC 곡선**입니다.  \n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_true = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
        "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        "roc_auc_score(y_true, y_scores)\n",
        "```\n",
        "\n",
        "- 시각화\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import accuracy_score, precision_score, precision_recall_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def learn_and_eval_clf(x, y_true):\n",
        "    clf = LogisticRegression(random_state=42)\n",
        "    clf.fit(x, y_true)\n",
        "    y_pred = clf.predict(x)\n",
        "    print(\"accuracy_score: {}\".format( accuracy_score(y_true, y_pred)))\n",
        "    print(\"precision_score: {}\".format( precision_score(y_true, y_pred)))\n",
        "\n",
        "    y_score = clf.decision_function(x)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "\n",
        "    f, axes = plt.subplots(1, 2, sharex=True, sharey=True)\n",
        "    f.set_size_inches((8, 4)) \n",
        "    axes[0].fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "    axes[0].set_title('Recall-Precision Curve')\n",
        "\n",
        "    axes[1].plot(fpr, tpr)\n",
        "    axes[1].plot([0, 1], [0, 1], linestyle='--')\n",
        "    axes[1].set_title('ROC curve')\n",
        "    #plt.save\n",
        "    return f\n",
        "\n",
        "## - 임의로 각각 평균이 0.0, 0.25인, 큰 차이가 나지 않는 샘플들을 뽑아서, 분류해본다. \n",
        "\n",
        "sample_size = 100\n",
        "x = np.vstack(\n",
        "    [np.random.normal(0, 1, sample_size*2).reshape(sample_size, 2), \n",
        "     np.random.normal(0.25, 1, sample_size*2).reshape(sample_size, 2), \n",
        "    ]\n",
        ")\n",
        "y_true = np.array([0 for i in range(0, sample_size)]+[1 for i in range(0, sample_size)])\n",
        "\n",
        "\n",
        "try1 = learn_and_eval_clf(x, y_true)\n",
        "\n",
        "\n",
        "### 임의로 각각 평균이 0.0, 2인, 큰 차이가 나지 않는 샘플들을 뽑아서, 분류해본다. \n",
        "\n",
        "sample_size = 100\n",
        "x = np.vstack(\n",
        "    [np.random.normal(0, 1, sample_size*2).reshape(sample_size, 2), \n",
        "     np.random.normal(2, 1, sample_size*2).reshape(sample_size, 2), \n",
        "    ]\n",
        ")\n",
        "y_true = np.array([0 for i in range(0, sample_size)]+[1 for i in range(0, sample_size)])\n",
        "try2 = learn_and_eval_clf(x, y_true)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShwcwsGVMHrp"
      },
      "source": [
        "> <h3> 📎 언제 ROC를 쓰고 언제 PR 곡선을 쓸까? </h3>\n",
        "\n",
        "- PR곡선: 일반적으로는 양성 클래스가 드물거나 FN보다 FP가 더 중요할 때\n",
        "  - e.g 5와 5가 아닌 숫자 구분 분류기\n",
        "- ROC곡선: 나머지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdpFMZREL4_F"
      },
      "source": [
        "> <h3> 📎 Confusion Matrix </h3>\n",
        "\n",
        "- 모델의 성능을 향상시키는 방법 중 하나로 에러의 종류를 분석하는 방법!\n",
        "- 가운데 대각선이 올바르게 분류된 것이고 나머지가 잘못 분류된 것들이다!\n",
        "- 오차행렬을 분석하면 classifier의 성능 향상 방안에 대한 통찰을 얻을 수 있다.\n",
        "\n",
        "![](https://www.harrisgeospatial.com/docs/html/images/Classification/ConfusionMatrix.gif)\n",
        "\n",
        "```python\n",
        "# confusion matrix 가장 기본적인 형태\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_train_5, y_train_pred)\n",
        "# confusion matrix 그려보기\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
        "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
        "conf_mx\n",
        "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
        "# color confusion matrix 그리는 방법\n",
        "def plot_confusion_matrix(matrix):\n",
        "    \"\"\"컬러 오차 행렬을 원할 경우\"\"\"\n",
        "    fig = plt.figure(figsize=(8,8))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(matrix)\n",
        "    fig.colorbar(cax)\n",
        "# 에러 비율을 비교하는 법! -> 개수로 비교하면 이미지가 많은 클래스가 상대적으로 나쁘게 보임\n",
        "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
        "norm_conf_mx = conf_mx / row_sums\n",
        "\n",
        "np.fill_diagonal(norm_conf_mx, 0)\n",
        "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
        "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM_DC19NbFQ9"
      },
      "source": [
        "## 🔎 3. Set Environment and Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4CTWK6bFWC"
      },
      "source": [
        "<h3> 🎈 1) Set Environment </h3>\n",
        "\n",
        "> Step.1 pip 설치하기\n",
        "- python이 저장된 경로\\Scripts에 pip.exe이 있다.\n",
        "- 이 pip을 가져와서 pip을 설치하자!\n",
        "```terminal\n",
        "$ pip3 install --upgrade pip\n",
        "```\n",
        "으로 최신버전 pip 업데이트!  \n",
        "\n",
        "> tep.2 필요한 패키지 깔기\n",
        "- \n",
        "```terminal\n",
        "$ pip3 install --upgrade jupyter matplotlib numpy pandas scipy scikit-learn\n",
        "```\n",
        "- \n",
        "```terminal\n",
        "$ python3 -c \"import jupyter matplotlib numpy pandas scipy scikit-learn\"\n",
        "```\n",
        "- [Mac 유저 개발환경 구축](https://subicura.com/2017/11/22/mac-os-development-environment-setup.html)  \n",
        "이 명령을 실행했을 때 어떤 에러, 메세지도 출력되지 않으면 성공!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iywps9zRbGlz"
      },
      "source": [
        "<h3> 🎈 2) Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNqo__Q-pksC"
      },
      "source": [
        "\n",
        "- External data, Internal data, Primary data, Secondary data\n",
        " - Internal data: Base Data(Application, warranty card), Channel data\n",
        " - External data: Survey, Consulting data, Alliance Data\n",
        " - Primary data: 현존하지 않고 만드는 데이터\n",
        " - Secondary data: 이미 내가 가지고 있거나 남이 가지고 있는 데이터\n",
        "- 크롤링, 공공포털 등에서 가져와도 되구~\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdTwmwdvbGua"
      },
      "source": [
        "<h3> 🎈 Split Data (Train, Test, Validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6pj3xjW_JRq"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "\n",
        "> train_test_split \n",
        "- test_size를 지정해주기만 하면 알아서 train과 test를 split함!\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "```  \n",
        "- numpy로 train, test 찢기\n",
        "```python\n",
        "import numpy as np\n",
        "shuffle_index = np.random.permutation(60000)\n",
        "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
        "```\n",
        "\n",
        "> StratifiedShuffleSplit\n",
        "- 그냥 train_test_split을 하면 label당 데이터가 고르게 섞이지 않을 수가 있다.\n",
        "- 그것을 피해 고르게 분배하기 위함~\n",
        "```python\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkchB18abGyc"
      },
      "source": [
        "## 🔎 4. Data EDA (Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6AJm-kdbGqV"
      },
      "source": [
        "- EDA (Exploratory Data Analysis, 탐색적 자료 분석): 데이터 분포는 어떤지, 상관관계는 어떤지, Outlier가 있는지, 데이터가 골고루 분포했는지 결측치가 많은지 등등\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHVla3OfbG2m"
      },
      "source": [
        "## 🔎 5. Data FE (Feature Engineering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9i6TESYbG6e"
      },
      "source": [
        "- FE는 굉장히 높은 전문성과 경험이 필요한 분야\n",
        "- FE에 대해서 자세하게 배우고 싶으면 2020-1 디자인팀 구글드라이브에 '[16기] 디자인팀 회칙 및 커리큘럼.docx'에 있는 Udemy FE강의를 참조할 것!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiAATuLcbG-b"
      },
      "source": [
        "### 🎈Categorical Data Hanlding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHPdS8GIkuz"
      },
      "source": [
        "<h4> 📎 Label encoding </h4> \n",
        "\n",
        "- 카테고리 변수를 다른 정숫값으로 매핑해주는 방법\n",
        "- ['17학번','15학번', '17학번', '16학번', '19학번']을  \n",
        "  [0, 1, 0, 2, 3]으로 매핑하는 것\n",
        "- Pandas의 factorize( ) 메서드를 사용하면 됨.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
        "housing_cat_encoded[:10]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QA1AESgyyOu"
      },
      "source": [
        "<h4> 📎 One-hot encoding </h4> \n",
        "\n",
        "- Label encoding의 단점? 컴퓨터가 못 알아먹음. 컴퓨터 입장에서는 Continuous Variable로 인식\n",
        "- 컴퓨터가 알아먹을 수 있게 하는 것이 One-hot encoding!\n",
        "- 컴퓨터가 이해할 수 있게 0, 1로만 표현하는 것  \n",
        "<br>\n",
        "\n",
        "![](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile27.uf.tistory.com%2Fimage%2F999790455B7D3327107DBD)\n",
        "- Pandas의 get_dummies( ) 메서드를 사용하면 됨.\n",
        "\n",
        "``` python\n",
        "# get_dummies() 예시\n",
        "df_dum = pd.get_dummies(df['pclass'])\n",
        "\n",
        "df_train = pd.get_dummies(df_train, prefix = ['Cabin'], columns=['Cabin'])\n",
        "\n",
        "df_dum = pd.get_dummies(df)\n",
        "\n",
        "temp = pd.get_dummies(temp, columns = ['Age_cut'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61q0ZkG2bHJx"
      },
      "source": [
        "### 🎈Scaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX0iUQnYdMA-"
      },
      "source": [
        "> Scaling이란.....\n",
        " - scaling을 통해서 다차원의 값들을 분석하기 쉽게 만들어주고 자료의 overflow, underflow를 방지할 수 있다! \n",
        " - 또한 최적화 과정에서의 안정성 및 수렴속도를 향상!\n",
        " - k-means, Gradient Descent 등 거리 기반의 모델에서는 scaling이 매우 중요하다\n",
        " - Regression(Normal Equation), Tree based model은 Scaling이 중요하지 않다.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "> 종류\n",
        " - StandardScaler: 기본 스케일, 평균과 표준편차 사용\n",
        " - MinMaxScaler: 최대/최소값이 각각 1, 0이 되도록 하는 scaling\n",
        " - MaxAbsScaler: 최대절대값과 0이 각각 1, 0이 되도록 하는 scaling\n",
        " - RobustScaler: median과 IQR(interquartile range) 사용, outlier의 영향을 최소화한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mcFkUKN_JRv"
      },
      "source": [
        "# StandardScaler\n",
        "## 예제1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "standardScaler = StandardScaler()\n",
        "print(standardScaler.fit(train_data))\n",
        "train_data_standardScaled = standardScaler.transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXlr3ZWv_JRw"
      },
      "source": [
        "# MinMaxscaler\n",
        "## 예제1\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minMaxScaler = MinMaxScaler()\n",
        "print(minMaxScaler.fit(train_data))\n",
        "train_data_minMaxScaled = minMaxScaler.transform(train_data)\n",
        "\n",
        "## 예시2\n",
        "minmax = MinMaxScaler()\n",
        "train_X_mm = minmax.fit_transform(train_X_nu)\n",
        "train_X_mm_df = pd.DataFrame(train_X_mm, columns=train_X_nu.columns, index=train_X_nu.index)\n",
        "train_X_fin = pd.concat([train_X_mm_df, train_X_ca], axis=1)\n",
        "train_X_fin.index = range(len(train_X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr3_GEa2_JRx"
      },
      "source": [
        "# MaxAbsScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "maxAbsScaler = MaxAbsScaler()\n",
        "print(maxAbsScaler.fit(train_data))\n",
        "train_data_maxAbsScaled = maxAbsScaler.transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFKt_Vhi_JRz"
      },
      "source": [
        "# RobustScaler 예시1\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "robustScaler = RobustScaler()\n",
        "print(robustScaler.fit(train_data))\n",
        "train_data_robustScaled = robustScaler.transform(train_data)\n",
        "\n",
        "# RobustScaler 예시2_pandas에 적용하기\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler = RobustScaler()\n",
        "temp[['psfMag_u', 'psfMag_g']] = scaler.fit_transform(temp[['psfMag_u', 'psfMag_g']])\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1SG_RNbIx5h"
      },
      "source": [
        "### 🎈Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBQYOfXHc_NH"
      },
      "source": [
        "> Oversampling\n",
        " - RandomOverSampler: 소수 클래스의 데이터를 반복해서 넣는 방법\n",
        "``` python\n",
        "X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - ADASYN: 소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법\n",
        "```python\n",
        "X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTE: ADASYN 방법처럼 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류한다.\n",
        "```python\n",
        "X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTENC: SMOTE와 유사하나 categorical 변수도 sampling 시킬 수 있는 방법\n",
        "```python\n",
        "X_res, y_res = SMOTENC(random_state=42, categorical_features=[18, 19]).fit_resample(X, y)\n",
        "```\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "> Undersampling\n",
        " - **RandomUnderSampler**: 무작위로 데이터를 없애는 단순 샘플링\n",
        "```python\n",
        "  X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **TomekLinks**: 토멕링크(클래스가 다른 두 데이터 중 아주 가까이 붙어있는 데이터)를 찾아서 그 중 다수 클래스에 속한 데이터를 제외하는 방법\n",
        "```python\n",
        "X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **CondensedNearestNeighbour**: 1-NN 모형으로 분류되지 않는 데이터만 남기는 방법\n",
        "```python\n",
        "X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **OneSidedSelection**: 토맥링크 중 다수 클래스를 제외하고 나머지 데이터 중에서도 서로 붙어있는 다수 클래스 데이터는 1-NN 방법으로 제외한다.\n",
        "```python\n",
        "X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **EditedNearestNeighbours**: 다수 클래스 데이터 중 가장 가까운 k(n_neighbors)개의 데이터가 모두(kind_sel=\"all\") 또는 다수(kind_sel=\"mode\") 다수 클래스가 아니면 삭제하는 방법이다. 소수 클래스 주변의 다수 클래스 데이터는 사라진다.\n",
        "```python\n",
        "X_samp, y_samp = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - **NeighbourhoodCleaningRule**: CNN(Condensed Nearest Neighbour) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.\n",
        "```python\n",
        "X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "> 복합샘플링\n",
        " - SMOTEENN: SMOTE + ENN\n",
        "```python\n",
        "X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "```\n",
        " - SMOTETomek: SMOTE + Tomek\n",
        "```python\n",
        "X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv9Uojbv_JR1"
      },
      "source": [
        "# SMOTE 코드\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)\n",
        "\n",
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n",
        "\n",
        "sm = SMOTE(random_state=2)\n",
        "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORd8tC4n_JR2"
      },
      "source": [
        "from imblearn.under_sampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXEVSVj4_JR3"
      },
      "source": [
        "# sampling 코드 모음\n",
        "X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTENC(random_state=42, categorical_features=[18, 19]).fit_resample(X_imb, y_imb)\n",
        "X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = EditedNearestNeighbours(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=\"all\", n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)\n",
        "X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlmT50pyI2Df"
      },
      "source": [
        "### 🎈Pipeline Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aka-FTuMc4jg"
      },
      "source": [
        "> pipline이란?\n",
        "- 연속된 변환을 순서대로 처리할수 있도록 도와주는 클래스임!\n",
        "- 연속된 단계를 나타내는 이름/estimator 쌍의 목록을 input으로 받음\n",
        "- 마지막 단계에는 estimator, transformer 모두 사용가능하며 그 외에는 모두 estimator  \n",
        "<br>  \n",
        "\n",
        "> ColumnTransformer란?\n",
        "- 두 pipeline을 하나의 pipeline으로 합치기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guM-pJBr_JR5"
      },
      "source": [
        "# pipeline 예시\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4DWvipV_JR7"
      },
      "source": [
        "# ColumnTransformer 예시\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num_pipeline\", num_pipeline, num_attribs),\n",
        "        (\"cat_encoder\", OneHotEncoder(categories='auto'), cat_attribs),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL1W4FCFbND6"
      },
      "source": [
        "## 🔎 6. Select Model and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRKCTwXbNHk"
      },
      "source": [
        "<h3> 🎈 Train and Evaluate Training Set </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ii82vIIh1El"
      },
      "source": [
        "- Train data로 훈련을 한 다음에 Test data로 평가를 하면 된다!\n",
        "- 평가 지표는 위에서 나온 지표 중에서 프로젝트에 제일 적합한 지표를 사용하면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U0ZrMfpbNLo"
      },
      "source": [
        "<h3> 🎈Evaluation with Cross Validation </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McV1bTkWxfr2"
      },
      "source": [
        "![datasets](https://t1.daumcdn.net/cfile/tistory/9951E5445AAE1BE025)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziYQkn1ExSNj"
      },
      "source": [
        "> Validation Set이 왜 필요할까?\n",
        " - Validation Set을 통해 우리는 모델의 성능을 대략적으로 파악할 수 있음\n",
        " - Train Set의 일부를 떼어낸 후, 남은 부분을 학습한 뒤, 모델을 통해 떼어낸 부분에 대한 예측 진행!\n",
        " - 우리는 떼어낸 부분의 실제 값을 알고 있기 때문에 예측치와 비교하여 성능을 평가하는 여러 측정 공식을 통해 모델의 성능을 측정함\n",
        " -  그러니 Validation Set은 모의고사 문제인 셈이쥬.  \n",
        " <br>\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt43DS3HdqRx"
      },
      "source": [
        "<h4> Cross Validation </h4>\n",
        "\n",
        "- Cross validation은 validation set을 뗄 만큼 data가 크지 않을 때 사용\n",
        "- Training Set을 여러 Subset으로 나누고 각 모델을 이 Subset의 조합으로 훈련시키고 나머지 부분으로 검증하는 방법이다.  \n",
        "![validation](https://t1.daumcdn.net/cfile/tistory/990DD2465B72F1491E)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2C7OG7O_JR_"
      },
      "source": [
        "# Cross validation code\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "X = np.arange(16).reshape((8,-1)) ## 8개의 row, 2개의 column\n",
        "y = np.arange(8).reshape((-1,1))\n",
        "kf = KFold(n_splits=4) # 8개의 row를 각각 2줄짜리 4개의 set으로 분리합니다\n",
        "for train_index, test_index in kf.split(X):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train, X_test = X[train_index], X[test_index] \n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "# 각각의 iteration을 print합니다. 첫 줄의 의미는 2~7번째 row를 학습하고 0,1번째 row를 validation set으로 하여 검증한다는 것입니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWVeBDgx_JSB"
      },
      "source": [
        "# Kfold 예시1\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=10)\n",
        "lasso_regressor = Lasso()\n",
        "ridge_regressor = Ridge()\n",
        "\n",
        "lasso_mse = []\n",
        "ridge_mse = []\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    lasso_regressor.fit(X[train_index], y[train_index])\n",
        "    ridge_regressor.fit(X[train_index], y[train_index])\n",
        "    \n",
        "    lasso_mse.append(mean_squared_error(y[test_index], lasso_regressor.predict(X[test_index])))\n",
        "    ridge_mse.append(mean_squared_error(y[test_index], ridge_regressor.predict(X[test_index])))\n",
        "    \n",
        "sum(lasso_mse) / 10, sum(ridge_mse) / 10\n",
        "\n",
        "################################################################################################################\n",
        "\n",
        "# Kfold 예시2\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np \n",
        "\n",
        "lasso_regressor = Lasso(warm_start=False)\n",
        "ridge_regressor = Ridge()\n",
        "\n",
        "lasso_scores = cross_val_score(lasso_regressor, X, y, cv=10, \n",
        "                               scoring='neg_mean_squared_error')\n",
        "ridge_scores= cross_val_score(ridge_regressor, X, y, cv=10, \n",
        "                              scoring='neg_mean_squared_error')\n",
        "# cv: 몇 번 돌릴 것이냐. cross-validation, scoring: 점수를 측정하는 기준\n",
        "np.mean(lasso_scores), np.mean(ridge_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47ggGGOObNPQ"
      },
      "source": [
        "## 🔎 7. Model Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUN69aHajLbb"
      },
      "source": [
        "- 모델이 복잡해지면 복잡해질수록 수많은 hyperparameter들이 있다.\n",
        "- Sklearn의 LinearRegression( )은 4개의 hyperparameter가 있는 반면 Xgboost는 40여개의 hyperparameter들이 있다.\n",
        "- 해결하려는 문제에 맞는 모델을 만들기 위해서는 Hyperparameter를 잘 조정해주면 된다!\n",
        "- Hyperparameter를 찾는 방법은 크게 2가지가 있다.\n",
        "  - GridSearchCV\n",
        "  - RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXxqASE8bNTj"
      },
      "source": [
        "### 🎈GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR3nDm1P_JSE"
      },
      "source": [
        "> GridSearchCV란?\n",
        "- 모델의 hyperparameter를 찾는 것을 도와주는 친구\n",
        "\n",
        "\n",
        "> 메소드 모음\n",
        "- **GridSearchCV ( estimator = , param_grid = , scoring = , cv = , n_jobs = , verbose = )** : grid search 하기\n",
        "- **.best_scores_**: 최적의 score 점수를 보여줌\n",
        "- **.best_estimator_**: 최적의 parameter로 설정된 모델을 생성\n",
        "- **.best_params_**: 최적의 parameter를 반환\n",
        "- **.cv_results_**: 전체적인 결과값들을 보여줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ovVFOM-_JSF"
      },
      "source": [
        "## Catboost best parameter 찾는 과정\n",
        "# 우선 부차적인 parameter 찾아주고\n",
        "cb = CatBoostClassifier(\n",
        " learning_rate =0.1,\n",
        " iterations=100 #n-estimator대신 iteration을 사용,\n",
        ")\n",
        "\n",
        "cb_params_1 = {\n",
        "    'depth' : [3,5,7],\n",
        "    'random_strength' : [1,3],\n",
        "    'bagging_temperature' : [0,0.5,1],\n",
        "    'l2_leaf_reg' : [1,3,5,7],\n",
        "}\n",
        "cb_grid_1 = GridSearchCV(cb, param_grid=cb_params_1, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_1.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_1.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_1.best_params_))\n",
        "best_cb_model = cb_grid_1.best_estimator_\n",
        "\n",
        "# 최적의 core parameter 찾기\n",
        "cb_params_2 = {\n",
        "    'learning_rate' : [0.03, 0.07, 0.1],\n",
        "    'iterations' : [n for n in range(80,130,20)]\n",
        "}\n",
        "cb_grid_2 = GridSearchCV(best_cb_model, param_grid=cb_params_2, scoring=my_scorer, cv=5, verbose=1)\n",
        "cb_grid_2.fit(train[features], train['Survived'])\n",
        "\n",
        "print(\"Best Score : {}\".format(cb_grid_2.best_score_))\n",
        "print(\"Best Params : {}\".format(cb_grid_2.best_params_))\n",
        "best_cb_model = cb_grid_2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ItofVWa_JSG"
      },
      "source": [
        "### Xgboost best parameter 찾기\n",
        "## 처음에는 부차적인 parameter를 tuning하기\n",
        "# GridSearchCV에 들어갈 param_grid, estimator, scoring 만들어주기\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import auc, f1_score, accuracy_score\n",
        "my_scorer = make_scorer(accuracy_score, greater_is_better = True)\n",
        "\n",
        "xgb1 = XGBClassifier(\n",
        " learning_rate =0.1,\n",
        " n_estimators=100,\n",
        ")\n",
        "\n",
        "xgb_params_1 = {\n",
        "    'max_depth' : [3,5,7],\n",
        "    'min_child_weight' : [0.5, 1],\n",
        "    'gamma' : [0, 0.1],\n",
        "    'subsample' : [0.5, 0.7, 0.9],\n",
        "    'colsample_bytree' : [0.5, 0.7, 0.9],\n",
        "} #3x2x2x3x3=108가지 경우의 수\n",
        "\n",
        "# GridSearchCV 돌리기\n",
        "xgb_grid_1 = GridSearchCV(estimator=xgb1, param_grid=xgb_params_1, \n",
        "                          scoring=my_scorer, cv=5, n_jobs=1, verbose=1)\n",
        "xgb_grid_1.fit(train[features], train['Survived']) # Titanic data\n",
        "\n",
        "# 제일 좋은 모델 뽑기\n",
        "print(\"Best Score : {}\".format(xgb_grid_1.best_score_))\n",
        "print(\"Best Params : {}\".format(xgb_grid_1.best_params_))\n",
        "\n",
        "# 표로 한 번 뽑아보고\n",
        "results = pd.DataFrame(xgb_grid_1.cv_results_)\n",
        "results = results.sort_values(by='mean_test_score', ascending=False)\n",
        "results.head()\n",
        "\n",
        "# 다시 Core parameter 설정!\n",
        "best_xgb_model = xgb_grid_1.best_estimator_\n",
        "xgb_params_2 = {\n",
        "    'learning_rate' : [0.01, 0.05, 0.07, 0.1, 0.2],\n",
        "    'n_estimators' : [n for n in range(100,200,20)]\n",
        "}\n",
        "xgb_grid_2 = GridSearchCV(best_xgb_model, param_grid=xgb_params_2, scoring=my_scorer, cv=5, verbose=1)\n",
        "xgb_grid_2.fit(train[features], train['Survived'])\n",
        "\n",
        "# 제일 좋은 모델 뽑기\n",
        "print(\"Best Score : {}\".format(xgb_grid_2.best_score_))\n",
        "print(\"Best Params : {}\".format(xgb_grid_2.best_params_))\n",
        "best_xgb_model = xgb_grid_2.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ghfBIcdbNXn"
      },
      "source": [
        "### 🎈RandomizedSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j419E81O_JSJ"
      },
      "source": [
        "> RandomizedSearchCV란?\n",
        "- GridSearchCV는 비교적 적은 수의 조합을 탐구할 때 괜찮음\n",
        "- 단 hyperparameter 탐색 공간이 커질 때는 RandomizedSearchCV가 더 유용\n",
        ">> 장점\n",
        "  - 탐색 1000회 -> 서로 다른 hyperparameter 조합 1000개 값 탐색\n",
        "  - 단순히 반복 횟수 조절하는 것으로도 컴퓨팅 자원 제어가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euVR9L3D_JSJ"
      },
      "source": [
        "### RandomizedSearchCV 예시 1\n",
        "# specify param\\eters and distributions to sample from\n",
        "param_dist = {'average': [True, False],\n",
        "              'l1_ratio': stats.uniform(0, 1),\n",
        "              'alpha': loguniform(1e-4, 1e0)}\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpietYbT_JSK"
      },
      "source": [
        "### RandomizedSearchCV 예시 2\n",
        "clf = BayesianRidge()\n",
        "clf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "#clf = RandomForestRegressor()\n",
        "if classifier:\n",
        "  clf = classifier\n",
        "  to_tune = False\n",
        "if to_tune:\n",
        "  # Grid search: find optimal classifier parameters.\n",
        "  param_grid = {'alpha_1': sp_rand(), 'alpha_2': sp_rand()}\n",
        "  param_grid = {'C': sp_rand(), 'gamma': sp_rand()}\n",
        "  rsearch = RandomizedSearchCV(estimator=clf, \n",
        "  param_distributions=param_grid, n_iter=5000)\n",
        "  rsearch.fit(X_train, y_train)\n",
        "  # Use tuned classifier.\n",
        "  clf = rsearch.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D2RtFha_JSM"
      },
      "source": [
        "### RandomizedSearchCV 예시 3\n",
        "n_estimators = [100, 200, 300, 400, 500]\n",
        "max_features = ['auto', 'sqrt']\n",
        "max_depth = [5, 10, 20, 30, 40, 50]\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        " model_tuned = RandomizedSearchCV(estimator = baseModel, \n",
        "                                  param_distributions = random_grid, \n",
        "                                  n_iter = 2, cv = 2, verbose=0, \n",
        "                                  random_state=42 , n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCYcjIVPIYNC"
      },
      "source": [
        "### 🎈 Bayesian optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSFEenxvJBhM"
      },
      "source": [
        "- 불필요한 hyperparameter 탐색시간을 줄여 보다 빠르게 최적 hyperparameter를 찾을 수 있는 알고리즘\n",
        "- GridSearchCV, RandomizedSearchCV 보다 더 좋은 성능을 내는 알고리즘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WolWZ4X6ppn8"
      },
      "source": [
        "#### 📁 Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMaunQSGZtxT"
      },
      "source": [
        "<h5>📌 BayesianOptimization with Logistic Regression <h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr1FJTjyZkYy"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def logreg_cv(C,\n",
        "          max_iter,\n",
        "          x_data=None, y_data=None, n_splits=5, output='score'):\n",
        "    \n",
        "    # K Fold 교차검증 만들기\n",
        "    score = 0\n",
        "    kf = KFold(n_splits=n_splits, random_state=4321)\n",
        "    models = []\n",
        "    for train_index, valid_index in kf.split(x_data):\n",
        "        x_train, y_train = x_data[train_index], y_data[train_index]\n",
        "        x_valid, y_valid = x_data[valid_index], y_data[valid_index]\n",
        "\n",
        "    # 모델 및 파라미터 정의\n",
        "        model = LogisticRegression(\n",
        "            C = C,\n",
        "            max_iter = int(max_iter)\n",
        "        )\n",
        "        \n",
        "        model.fit(x_train, y_train)\n",
        "        models.append(model)\n",
        "        \n",
        "    # Evaluate model with average f1 score    \n",
        "        pred = model.predict(x_valid)\n",
        "        true = y_valid\n",
        "        score += accuracy_score(true, pred)/n_splits\n",
        "    \n",
        "    if output == 'score':\n",
        "        return score\n",
        "    if output == 'model':\n",
        "        return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyGIyn2oHNW"
      },
      "source": [
        "<h5> 📌 BayesianOptimization with XGBoost (Binary Class) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvokIum-oJvI"
      },
      "source": [
        "def XGB_CV(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate\n",
        "         ):\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "    \n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "    \n",
        "    # Model defining\n",
        "    xgb = XGBClassifier(max_depth = int(max_depth),\n",
        "                        gamma = gamma,\n",
        "                        learning_rate = learning_rate,\n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0),\n",
        "                        min_child_weight = min_child_weight,\n",
        "                        max_delta_step = int(max_delta_step),\n",
        "                        n_estimators = 20000,\n",
        "                        random_state=42, \n",
        "#                         tree_method='gpu_hist' ,\n",
        "                       )\n",
        "    \n",
        "    # Model Training\n",
        "    xgb.fit(X_train, y_train,\n",
        "            early_stopping_rounds = 100,\n",
        "            eval_metric=[\"auc\"], verbose=False,\n",
        "            eval_set=[(X_test, y_test)])\n",
        "    \n",
        "    \n",
        "    val_score = max(xgb.evals_result()['validation_0']['auc'])\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(xgb.evals_result()['validation_0']['auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(xgb.evals_result()['validation_0']['auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4uOHWLfoJsX"
      },
      "source": [
        "X = titanic.iloc[:, 1:]; y = titanic.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfcwJ2jNoJpj"
      },
      "source": [
        "XGB_BO = BayesianOptimization(XGB_CV,pbounds= {\n",
        "                                    'max_depth': (2, 12),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1), \n",
        "                                    }, \n",
        "                               verbose= 2,\n",
        "                               random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHe0HmH5oJml"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    XGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh1N8qcHoJjj"
      },
      "source": [
        "# Best hyperparameter\n",
        "print(XGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6omOliWZz11"
      },
      "source": [
        "<h5>📌  BayesianOptimization with XGBoost (Multiclass) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwGYcThtoDNC"
      },
      "source": [
        "def custom_eval(pred, dtrain):\n",
        "  labels = dtrain.get_label()\n",
        "  lb = LabelBinarizer()\n",
        "  lb.fit(labels)\n",
        "  label = lb.transform(labels)\n",
        "  return 'roc_auc' , -roc_auc_score(label, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46sud6xHZsNg"
      },
      "source": [
        "def XGB_CV(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    # Model defining\n",
        "    xgb = XGBClassifier(max_depth = int(max_depth),\n",
        "                        gamma = gamma,\n",
        "                        learning_rate = learning_rate,\n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0),\n",
        "                        min_child_weight = min_child_weight,\n",
        "                        max_delta_step = int(max_delta_step),\n",
        "                        n_estimators = 20000,\n",
        "                        random_state=42, \n",
        "#                         tree_method='gpu_hist' ,\n",
        "                        silent=True)\n",
        "    \n",
        "    # Model Training\n",
        "    xgb.fit(X_train, y_train,\n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)], \n",
        "            eval_metric=custom_eval, verbose=False)\n",
        "    \n",
        "    val_score = -xgb.evals_result()['validation_0']['roc_auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(xgb.evals_result()['validation_0']['roc_auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(xgb.evals_result()['validation_0']['roc_auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBdu4qnEoDt0"
      },
      "source": [
        "XGB_BO = BayesianOptimization(XGB_CV,pbounds= {\n",
        "                                    'max_depth': (2, 12),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1),                                  \n",
        "                                    }, \n",
        "                               verbose= 2,\n",
        "                               random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTq_LObUoFIE"
      },
      "source": [
        "X = df_nn.iloc[:, :-1].values; y = df_nn.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpRqFQwQoFNK"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    XGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTIvSx2noFSS"
      },
      "source": [
        "# Best hyperparameter\n",
        "print(XGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcfcRVzlp_s5"
      },
      "source": [
        "<h5>📌  BayesianOptimization with LightGBM (Binary Class) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se1_eLcAqDe_"
      },
      "source": [
        "def LGB_opt(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "\n",
        "    lgb = LGBMClassifier(class_weight=None,\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0), \n",
        "                        learning_rate = learning_rate,\n",
        "                        #gamma = gamma, \n",
        "                        max_depth = int(max_depth), \n",
        "                        min_child_weight = min_child_weight, \n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        num_leaves = min(2**int(max_depth),131072),\n",
        "                        n_estimators = 20000, \n",
        "                        n_jobs=-1, \n",
        "#                         device='gpu',\n",
        "                        random_state=42)\n",
        "                        \n",
        "    lgb.fit(X_train, y_train,\n",
        "            verbose=False,  \n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_metric=['auc'])\n",
        "\n",
        "    val_score = lgb.evals_result_['valid_0']['auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(lgb.evals_result_['valid_0']['auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(lgb.evals_result_['valid_0']['auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-a3nwBqDlU"
      },
      "source": [
        "X = titanic.iloc[:, 1:]; y = titanic.iloc[:, 0]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddlzzQTQqDiN"
      },
      "source": [
        "LGB_BO = BayesianOptimization(LGB_opt, {\n",
        "                                    'max_depth': (2, 20),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1)\n",
        "                                    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghd9NS1qDoB"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRF62gLqUaQ"
      },
      "source": [
        "print(LGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgpc6KkFp_wD"
      },
      "source": [
        "<h5>📌  BayesianOptimization with LightGBM (Multiclass) </h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGO1AoxGqHQh"
      },
      "source": [
        "def LGB_opt(max_depth,\n",
        "           gamma,\n",
        "           min_child_weight,\n",
        "           max_delta_step,\n",
        "           subsample,\n",
        "           colsample_bytree,\n",
        "           learning_rate,\n",
        "         ):\n",
        "\n",
        "    global AUCbest\n",
        "    global ITERbest\n",
        "\n",
        "    folds = 10\n",
        "    cv_score = 0\n",
        "\n",
        "    lgb = LGBMClassifier(class_weight=None,\n",
        "                        colsample_bytree = max(min(colsample_bytree, 1), 0), \n",
        "                        learning_rate = learning_rate,\n",
        "                        #gamma = gamma, \n",
        "                        max_depth = int(max_depth), \n",
        "                        min_child_weight = min_child_weight, \n",
        "                        subsample = max(min(subsample, 1), 0),\n",
        "                        num_leaves = min(2**int(max_depth),131072),\n",
        "                        n_estimators = 20000, \n",
        "                        n_jobs=-1, \n",
        "                        device='gpu',\n",
        "                        random_state=42)\n",
        "                        \n",
        "    lgb.fit(X_train, y_train,\n",
        "            verbose=False,  \n",
        "            early_stopping_rounds = 100,\n",
        "            eval_set=[(X_test, y_test)],\n",
        "            eval_metric=custom_eval)\n",
        "\n",
        "    val_score = lgb.evals_result_['valid_0']['roc_auc'][-1]\n",
        "    print(' Stopped after %d iterations with val-auc = %f val-gini = %f' % ( len(lgb.evals_result_['valid_0']['roc_auc']), val_score, (val_score*2-1)) )\n",
        "    if ( val_score > AUCbest ):\n",
        "        AUCbest = val_score\n",
        "        ITERbest = len(lgb.evals_result_['valid_0']['roc_auc'])\n",
        "\n",
        "    return (val_score*2) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFc9biywqHTm"
      },
      "source": [
        "def custom_eval(y_true, y_pred):\n",
        "  preds = y_pred.reshape(11, -1).T\n",
        "  lb = LabelBinarizer()\n",
        "  lb.fit(y_true)\n",
        "  label = lb.transform(y_true)\n",
        "  return 'roc_auc' , roc_auc_score(label, preds), True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me21tnUgqHXQ"
      },
      "source": [
        "LGB_BO = BayesianOptimization(LGB_opt, {\n",
        "                                    'max_depth': (2, 20),\n",
        "                                    'gamma': (0.001, 10.0),\n",
        "                                    'min_child_weight': (0, 20),\n",
        "                                    'max_delta_step': (0, 10),\n",
        "                                    'subsample': (0.4, 1.0),\n",
        "                                    'colsample_bytree' :(0.4, 1.0),\n",
        "                                    'learning_rate' : (0.01, 0.1)\n",
        "                                    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPRRU-zvqK4U"
      },
      "source": [
        "df = pd.read_csv(path+test_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe5dKYJbqHaT"
      },
      "source": [
        "X = df.iloc[:, 1:].values; y = df.iloc[:, 0].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rczS8eDIqHdK"
      },
      "source": [
        "AUCbest = -1.\n",
        "ITERbest = 0\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings('ignore')\n",
        "    LGB_BO.maximize(init_points=2, n_iter=30, acq='ei', xi=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESQeSpcGqHf2"
      },
      "source": [
        "print(LGB_BO.max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJukgb2bNbQ"
      },
      "source": [
        "<h3> 🎈 Ensemble </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcVpF-pzms4N"
      },
      "source": [
        "- \"약한 모델을 여러 개 합치면 강한 모델 1개보다 성능이 좋다!\"\n",
        "- 이게 앙상블의 아이디어!\n",
        "- 5월에 배우게 될 거예요!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYSuxKNWbNfE"
      },
      "source": [
        "<h3> 🎈 Best Model and Error Analysis </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPSEN3UOnDVq"
      },
      "source": [
        "- 가장 적합한 모델을 분석하면 문제에 대한 좋은 insight들을 많이 얻기도 한다.\n",
        "- 예) Feature_importances_를 사용하면 어떤 feature가 모델에 영향을 많이 미치는지 알려준다! 즉 해결하고자 하는 문제에서 어떤 특성들이 중요한지 상대적 중요도를 알려준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCdGnJyUnalB"
      },
      "source": [
        "## Random Forest Feature importance\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49rmjjecnaUn"
      },
      "source": [
        "## Xgboost Plot importance\n",
        "# 코드1\n",
        "from xgboost import plot_importance\n",
        "\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X, y)\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "for index in sorted_idx:\n",
        "    print([X.columns[index], model.feature_importances_[index]])\n",
        "\n",
        "# 코드2\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "model = XGBClassifier()\n",
        "model.fit(train, label)\n",
        "\n",
        "sorted_idx = np.argsort(model.feature_importances_)[::-1]\n",
        "\n",
        "for index in sorted_idx:\n",
        "    print([train.columns[index], model.feature_importances_[index]]) \n",
        "\n",
        "    plot_importance(model, max_num_features = 15)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEzSnmlTbNjN"
      },
      "source": [
        "<h3> 🎈 Evaluate System with Test Set </h3>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUxy8_jn5Nw"
      },
      "source": [
        "- Train set과 Validation set으로 훈련시킨 모델을 이제 Test set에 적용해서 score를 보면 된다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBQ_pbhmbTXV"
      },
      "source": [
        "## 🔎 8. Launch, Monitoring, and System Maintenance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAL0mzN7oE4V"
      },
      "source": [
        "- 이제 해당 모델을 적용시켜서 문제 해결에 적극 활용하면 된다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJWmUSDlAyOK"
      },
      "source": [
        "# ✏️ Useful Machine Learning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjjuHB0rwULR"
      },
      "source": [
        "<h3> 🔎 How to Save Models</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_W-Sdk6w8vO"
      },
      "source": [
        "Use **joblib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZLxjgBNOXCf"
      },
      "source": [
        "## 저장할 때\n",
        "from sklearn.externals import joblib \n",
        "# 객체를 pickled binary file 형태로 저장한다 \n",
        "file_name = 'object_01.pkl' \n",
        "joblib.dump(obj, file_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlqZIRBCwWwI"
      },
      "source": [
        "## 읽을 때\n",
        "from sklearn.externals import joblib \n",
        "# pickled binary file 형태로 저장된 객체를 로딩한다 \n",
        "file_name = 'object_01.pkl' \n",
        "obj = joblib.load(file_name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_CciL_ew_OG"
      },
      "source": [
        "Using **pickle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvMC4llKwA2_"
      },
      "source": [
        "# Save Model Using Pickle\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Fit the model on training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        " \n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}